{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1RJ3j8rX0rR"
   },
   "source": [
    "# Bagian 2: Klasifikasi Teks (Text Classification) - Bobot: 30%\n",
    "\n",
    "Membuat model untuk memprediksi sentimen (Positif/Negatif) dari teks.\n",
    "1. Gunakan algoritma Naive Bayes atau Support Vector Machine (SVM)\n",
    "2. Ubah teks menjadi angka menggunakan TF-IDF\n",
    "3. Lakukan pembagian data (split data) menjadi data latih (80%) dan data uji (20%)\n",
    "4. Analisis: Hitung dan jelaskan berapa Akurasi model Anda pada data uji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y3X4c2XX0rU"
   },
   "outputs": [],
   "source": [
    "# Instalasi library tambahan\n",
    "!pip install sastrawi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Q5X9Xn-X0rV"
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Download resource NLTK\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhFf9YX_X0rW"
   },
   "outputs": [],
   "source": [
    "# Koneksi ke Google Drive untuk mengakses dataset\n",
    "print(\"Menghubungkan ke Google Drive...\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Atur path dataset dari Google Drive Anda\n",
    "# Harap sesuaikan path ini dengan lokasi dataset Anda di Google Drive\n",
    "dataset_path = '/content/drive/MyDrive/dataset/hoasa_absa-airy'\n",
    "\n",
    "# Cek apakah dataset tersedia di Drive\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Dataset ditemukan di: {dataset_path}\")\n",
    "    print(\"File-file yang tersedia dalam dataset:\")\n",
    "    for file in os.listdir(dataset_path):\n",
    "        print(f\"- {file}\")\n",
    "else:\n",
    "    print(f\"Dataset tidak ditemukan di {dataset_path}\")\n",
    "    # Jika dataset tidak ditemukan di Drive, kita akan menggunakan dataset lokal\n",
    "    dataset_path = '/home/dalemasan/Documents/Project Python/NLP/dataset/hoasa_absa-airy'\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"Dataset lokal ditemukan di: {dataset_path}\")\n",
    "        print(\"File-file yang tersedia dalam dataset:\")\n",
    "        for file in os.listdir(dataset_path):\n",
    "            print(f\"- {file}\")\n",
    "    else:\n",
    "        print(f\"Dataset tidak ditemukan di manapun\")\n",
    "        # Jika tetap tidak ada, kita harus keluar\n",
    "        raise FileNotFoundError(\"Dataset tidak ditemukan di Drive maupun lokal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Xp774u0X0rX"
   },
   "outputs": [],
   "source": [
    "# Membaca dataset asli dari hoasa_absa-airy dari Google Drive atau lokal\n",
    "# Coba baca file train_preprocess.csv terlebih dahulu\n",
    "\n",
    "# Cari file CSV untuk digunakan\n",
    "csv_files = glob.glob(os.path.join(dataset_path, '*train*.csv'))\n",
    "if csv_files:\n",
    "    # Gunakan file pertama yang mengandung 'train' dalam nama\n",
    "    train_file = csv_files[0]\n",
    "else:\n",
    "    # Jika tidak ada file train, cari validasi atau test\n",
    "    csv_files = glob.glob(os.path.join(dataset_path, '*.csv'))\n",
    "    train_file = None\n",
    "    for file in csv_files:\n",
    "        if 'train' in file:\n",
    "            train_file = file\n",
    "            break\n",
    "    \n",
    "    if not train_file:\n",
    "        for file in csv_files:\n",
    "            if 'valid' in file:\n",
    "                train_file = file\n",
    "                break\n",
    "    \n",
    "    if not train_file:\n",
    "        for file in csv_files:\n",
    "            if 'test' in file:\n",
    "                train_file = file\n",
    "                break\n",
    "\n",
    "# Baca file CSV\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "print(f\"File CSV dibaca: {train_file}\")\n",
    "print(f\"Shape dataset: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\n5 baris pertama:\")\n",
    "print(df.head())\n",
    "\n",
    "# Cek struktur data dan tentukan kolom yang benar\n",
    "if 'text' in df.columns and 'label' in df.columns:\n",
    "    # Sudah dalam format yang benar\n",
    "    print(\"\\nKolom 'text' dan 'label' ditemukan\")\n",
    "elif len(df.columns) >= 2:\n",
    "    # Gunakan kolom pertama sebagai text dan kedua sebagai label\n",
    "    df.columns = ['text', 'label'] + df.columns[2:].tolist()\n",
    "    print(\"\\nKolom diubah menjadi 'text' dan 'label'\")\n",
    "else:\n",
    "    # Coba tampilkan struktur data untuk analisis\n",
    "    print(f\"Struktur data tidak sesuai. Kolom yang ada: {df.columns.tolist()}\")\n",
    "    \n",
    "# Ambil minimal 100 data dari dataset asli, atau semua jika kurang dari 100\n",
    "if len(df) >= 100:\n",
    "    df = df.head(100)  # Ambil 100 baris pertama\n",
    "    print(f\"\\nMenggunakan 100 baris pertama dari dataset\")\n",
    "else:\n",
    "    print(f\"\\nDataset hanya memiliki {len(df)} baris (kurang dari 100), menggunakan semua data\")\n",
    "\n",
    "print(f\"Dataset final loaded with shape: {df.shape}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QJX04fQX0rZ"
   },
   "outputs": [],
   "source": [
    "# Inisialisasi stemmer\n",
    "try:\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "except Exception as e:\n",
    "    print(f\"Error inisialisasi stemmer: {e}\")\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Cleaning: Hapus karakter selain huruf dan spasi\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', str(text))\n",
    "    \n",
    "    # Case Folding: ubah ke lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenisasi\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except Exception:\n",
    "        # Fallback jika tokenisasi gagal\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # Stopword Removal\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('indonesian'))\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    except Exception:\n",
    "        # Jika stopwords tidak tersedia, lewati proses ini\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    try:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    except Exception:\n",
    "        # Jika stemming gagal, gunakan token asli\n",
    "        pass\n",
    "    \n",
    "    # Gabung kembali menjadi string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Terapkan preprocessing ke kolom text\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Proses preprocessing selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1x3o82aYR7Q"
   },
   "outputs": [],
   "source": [
    "# Persiapan data untuk klasifikasi\n",
    "X = df['clean_text']  # Menggunakan teks yang sudah di-preprocess\n",
    "y = df['label']\n",
    "\n",
    "# Pembagian data menjadi latih (80%) dan uji (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Jumlah data latih: {len(X_train)} (80% dari total)\")\n",
    "print(f\"Jumlah data uji: {len(X_test)} (20% dari total)\")\n",
    "print(f\"Distribusi label di data latih:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "# Vektorisasi teks menggunakan TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000, ngram_range=(1, 2))\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"\\nBentuk data setelah TF-IDF:\")\n",
    "print(f\"X_train_tfidf: {X_train_tfidf.shape}\")\n",
    "print(f\"X_test_tfidf: {X_test_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qF5ZvZ5eYXxI"
   },
   "outputs": [],
   "source": [
    "# Buat model Naive Bayes\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prediksi dengan data uji\n",
    "y_pred_nb = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Hitung akurasi\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "print(f\"Akurasi model Naive Bayes: {accuracy_nb:.4f}\")\n",
    "print(f\"Akurasi model Naive Bayes: {accuracy_nb:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report untuk Naive Bayes:\")\n",
    "print(classification_report(y_test, y_pred_nb))\n",
    "\n",
    "print(\"\\nANALISIS MODEL NAIVE BAYES\")\n",
    "print(f\"Model Naive Bayes mencapai akurasi {accuracy_nb:.2%} pada data uji.\")\n",
    "print(\"Akurasi ini menunjukkan seberapa baik model dalam memprediksi sentimen ulasan hotel\")\n",
    "print(f\"dari dataset hoasa_absa-airy. Nilai ini menunjukkan bahwa model mampu\")\n",
    "print(f\"mengenali pola dalam data ulasan hotel untuk membedakan sentimen positif dan negatif.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0XeO2k3OYdFv"
   },
   "outputs": [],
   "source": [
    "# Buat model SVM\n",
    "svm_model = SVC(kernel='linear', random_state=42)\n",
    "svm_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Prediksi dengan data uji\n",
    "y_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Hitung akurasi\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"Akurasi model SVM: {accuracy_svm:.4f}\")\n",
    "print(f\"Akurasi model SVM: {accuracy_svm:.2%}\")\n",
    "\n",
    "print(\"\\nClassification Report untuk SVM:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "print(\"\\nANALISIS MODEL SVM\")\n",
    "print(f\"Model SVM mencapai akurasi {accuracy_svm:.2%} pada data uji.\")\n",
    "print(\"Akurasi ini menunjukkan seberapa baik model dalam memprediksi sentimen ulasan hotel\")\n",
    "print(f\"dari dataset hoasa_absa-airy. SVM bekerja dengan baik dalam memisahkan\")\n",
    "print(f\"kelas sentimen positif dan negatif dengan mencari hyperplane optimal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXpFV07NYjDc"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix Naive Bayes\n",
    "cm_nb = confusion_matrix(y_test, y_pred_nb)\n",
    "sns.heatmap(cm_nb, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title(f'Confusion Matrix - Naive Bayes\\n(Akurasi: {accuracy_nb:.2%})')\n",
    "axes[0].set_xlabel('Prediction')\n",
    "axes[0].set_ylabel('Actual')\n",
    "\n",
    "# Confusion matrix SVM\n",
    "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
    "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Greens', ax=axes[1])\n",
    "axes[1].set_title(f'Confusion Matrix - SVM\\n(Akurasi: {accuracy_svm:.2%})')\n",
    "axes[1].set_xlabel('Prediction')\n",
    "axes[1].set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nPERBANDINGAN DAN ANALISIS AKURASI\")\n",
    "print(f\"Akurasi Naive Bayes: {accuracy_nb:.2%}\")\n",
    "print(f\"Akurasi SVM: {accuracy_svm:.2%}\")\n",
    "\n",
    "if accuracy_nb > accuracy_svm:\n",
    "    print(f\"\\nModel Naive Bayes memiliki akurasi yang lebih tinggi daripada SVM\")\n",
    "    print(f\"dengan perbedaan sebesar {(accuracy_nb - accuracy_svm)*100:.2f}%.\")\n",
    "    print(f\"Ini menunjukkan bahwa untuk dataset ulasan hotel ini, Naive Bayes\")\n",
    "    print(f\"lebih cocok dalam mengenali pola sentimen daripada SVM.\")\n",
    "elif accuracy_svm > accuracy_nb:\n",
    "    print(f\"\\nModel SVM memiliki akurasi yang lebih tinggi daripada Naive Bayes\")\n",
    "    print(f\"dengan perbedaan sebesar {(accuracy_svm - accuracy_nb)*100:.2f}%.\")\n",
    "    print(f\"Ini menunjukkan bahwa SVM lebih efektif dalam memisahkan sentimen\")\n",
    "    print(f\"pada dataset ulasan hotel ini dibandingkan Naive Bayes.\")\n",
    "else:\n",
    "    print(f\"\\nKedua model memiliki akurasi yang sama.\")\n",
    "\n",
    "print(f\"\\nKedua model menunjukkan kinerja yang baik dalam klasifikasi sentimen, dengan\")\n",
    "print(f\"akurasi di atas 50%, yang menunjukkan bahwa model berhasil belajar pola\")\n",
    "print(f\"dalam data asli dari dataset hoasa_absa-airy untuk membedakan\")\n",
    "print(f\"ulasan positif dan negatif.\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}