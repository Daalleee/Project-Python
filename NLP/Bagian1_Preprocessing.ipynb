{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1RJ3j8rX0rR"
   },
   "source": [
    "# Bagian 1: Pra-pemrosesan Teks (Bobot: 20%)\n",
    "\n",
    "Sebelum data diolah, data harus dibersihkan agar model dapat bekerja maksimal. Teknik preprocessing yang digunakan:\n",
    "1. Case Folding: Mengubah huruf menjadi kecil semua\n",
    "2. Tokenisasi: Memecah kalimat menjadi kata-kata\n",
    "3. Stopword Removal: Menghapus kata umum yang tidak bermakna\n",
    "4. Cleaning: Menghapus tanda baca, angka, atau karakter aneh menggunakan Regex\n",
    "5. Stemming: Mengubah kata ke bentuk dasarnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Y3X4c2XX0rU"
   },
   "outputs": [],
   "source": [
    "# Instalasi library tambahan\n",
    "!pip install sastrawi -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Q5X9Xn-X0rV"
   },
   "outputs": [],
   "source": [
    "# Import library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Download resource NLTK\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error downloading NLTK resources: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhFf9YX_X0rW"
   },
   "outputs": [],
   "source": [
    "# Koneksi ke Google Drive untuk mengakses dataset\n",
    "print(\"Menghubungkan ke Google Drive...\")\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Atur path dataset dari Google Drive Anda\n",
    "# Harap sesuaikan path ini dengan lokasi dataset Anda di Google Drive\n",
    "dataset_path = '/content/drive/MyDrive/dataset/hoasa_absa-airy'\n",
    "\n",
    "# Cek apakah dataset tersedia di Drive\n",
    "if os.path.exists(dataset_path):\n",
    "    print(f\"Dataset ditemukan di: {dataset_path}\")\n",
    "    print(\"File-file yang tersedia dalam dataset:\")\n",
    "    for file in os.listdir(dataset_path):\n",
    "        print(f\"- {file}\")\n",
    "else:\n",
    "    print(f\"Dataset tidak ditemukan di {dataset_path}\")\n",
    "    # Jika dataset tidak ditemukan di Drive, kita akan menggunakan dataset lokal\n",
    "    dataset_path = '/home/dalemasan/Documents/Project Python/NLP/dataset/hoasa_absa-airy'\n",
    "    if os.path.exists(dataset_path):\n",
    "        print(f\"Dataset lokal ditemukan di: {dataset_path}\")\n",
    "        print(\"File-file yang tersedia dalam dataset:\")\n",
    "        for file in os.listdir(dataset_path):\n",
    "            print(f\"- {file}\")\n",
    "    else:\n",
    "        print(f\"Dataset tidak ditemukan di manapun\")\n",
    "        # Jika tetap tidak ada, kita harus keluar\n",
    "        raise FileNotFoundError(\"Dataset tidak ditemukan di Drive maupun lokal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Xp774u0X0rX"
   },
   "outputs": [],
   "source": [
    "# Membaca dataset asli dari hoasa_absa-airy dari Google Drive atau lokal\n",
    "# Coba baca file train_preprocess.csv terlebih dahulu\n",
    "\n",
    "# Cari file CSV untuk digunakan\n",
    "csv_files = glob.glob(os.path.join(dataset_path, '*train*.csv'))\n",
    "if csv_files:\n",
    "    # Gunakan file pertama yang mengandung 'train' dalam nama\n",
    "    train_file = csv_files[0]\n",
    "else:\n",
    "    # Jika tidak ada file train, cari validasi atau test\n",
    "    csv_files = glob.glob(os.path.join(dataset_path, '*.csv'))\n",
    "    train_file = None\n",
    "    for file in csv_files:\n",
    "        if 'train' in file:\n",
    "            train_file = file\n",
    "            break\n",
    "    \n",
    "    if not train_file:\n",
    "        for file in csv_files:\n",
    "            if 'valid' in file:\n",
    "                train_file = file\n",
    "                break\n",
    "    \n",
    "    if not train_file:\n",
    "        for file in csv_files:\n",
    "            if 'test' in file:\n",
    "                train_file = file\n",
    "                break\n",
    "\n",
    "# Baca file CSV\n",
    "df = pd.read_csv(train_file)\n",
    "\n",
    "print(f\"File CSV dibaca: {train_file}\")\n",
    "print(f\"Shape dataset: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\n5 baris pertama:\")\n",
    "print(df.head())\n",
    "\n",
    "# Cek struktur data dan tentukan kolom yang benar\n",
    "if 'text' in df.columns and 'label' in df.columns:\n",
    "    # Sudah dalam format yang benar\n",
    "    print(\"\\nKolom 'text' dan 'label' ditemukan\")\n",
    "elif len(df.columns) >= 2:\n",
    "    # Gunakan kolom pertama sebagai text dan kedua sebagai label\n",
    "    df.columns = ['text', 'label'] + df.columns[2:].tolist()\n",
    "    print(\"\\nKolom diubah menjadi 'text' dan 'label'\")\n",
    "else:\n",
    "    # Coba tampilkan struktur data untuk analisis\n",
    "    print(f\"Struktur data tidak sesuai. Kolom yang ada: {df.columns.tolist()}\")\n",
    "    \n",
    "# Ambil minimal 100 data dari dataset asli, atau semua jika kurang dari 100\n",
    "if len(df) >= 100:\n",
    "    df = df.head(100)  # Ambil 100 baris pertama\n",
    "    print(f\"\\nMenggunakan 100 baris pertama dari dataset\")\n",
    "else:\n",
    "    print(f\"\\nDataset hanya memiliki {len(df)} baris (kurang dari 100), menggunakan semua data\")\n",
    "\n",
    "print(f\"Dataset final loaded with shape: {df.shape}\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y978Zy8bX0rY"
   },
   "outputs": [],
   "source": [
    "print(\"Data sebelum preprocessing (5 baris pertama dari dataset asli):\")\n",
    "df_before = df[['text', 'label']].head()\n",
    "display(df_before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8QJX04fQX0rZ"
   },
   "outputs": [],
   "source": [
    "# Inisialisasi stemmer\n",
    "try:\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "except Exception as e:\n",
    "    print(f\"Error inisialisasi stemmer: {e}\")\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Cleaning: Hapus karakter selain huruf dan spasi\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', str(text))\n",
    "    \n",
    "    # Case Folding: ubah ke lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenisasi\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except Exception:\n",
    "        # Fallback jika tokenisasi gagal\n",
    "        tokens = text.split()\n",
    "    \n",
    "    # Stopword Removal\n",
    "    try:\n",
    "        stop_words = set(stopwords.words('indonesian'))\n",
    "        tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    except Exception:\n",
    "        # Jika stopwords tidak tersedia, lewati proses ini\n",
    "        tokens = [token for token in tokens if len(token) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    try:\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "    except Exception:\n",
    "        # Jika stemming gagal, gunakan token asli\n",
    "        pass\n",
    "    \n",
    "    # Gabung kembali menjadi string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Terapkan preprocessing ke kolom text\n",
    "df['clean_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "print(\"Proses preprocessing selesai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "23Y60vLlX0rZ"
   },
   "outputs": [],
   "source": [
    "print(\"Data setelah preprocessing (5 baris pertama):\")\n",
    "df_after = df[['text', 'clean_text', 'label']].head()\n",
    "display(df_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK37J28cX0ra"
   },
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "TUGAS LAPORAN: Menjelaskan mengapa preprocessing penting dilakukan\n",
    "\n",
    "Tahap preprocessing penting dilakukan sebelum masuk ke tahap machine learning karena:\n",
    "\n",
    "1. Case Folding: Menghindari perbedaan antara kata yang sama dengan kapitalisasi berbeda \n",
    "   (contoh: 'Hotel' vs 'hotel'), sehingga model tidak menganggapnya sebagai kata berbeda\n",
    "\n",
    "2. Tokenisasi: Memungkinkan komputer memproses teks sebagai unit-unit kecil (kata-kata) \n",
    "   daripada string panjang, memudahkan analisis dan pemrosesan\n",
    "\n",
    "3. Stopword Removal: Mengurangi noise dari kata-kata umum yang tidak memberi informasi\n",
    "   signifikan (seperti 'yang', 'dan', 'di'), sehingga model fokus pada kata-kata penting\n",
    "\n",
    "4. Cleaning: Membersihkan karakter aneh, tanda baca, atau angka yang bisa mengganggu\n",
    "   proses machine learning, membuat representasi teks lebih konsisten\n",
    "\n",
    "5. Stemming: Mengurangi variasi kata ke bentuk dasarnya (contoh: 'membangun' -> 'bangun'),\n",
    "   mengurangi dimensi fitur dan membuat model lebih efisien\n",
    "\n",
    "Dengan preprocessing yang baik, data menjadi lebih bersih, terstruktur, dan siap diproses\n",
    "oleh algoritma machine learning, yang akan meningkatkan akurasi dan kinerja model.\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}